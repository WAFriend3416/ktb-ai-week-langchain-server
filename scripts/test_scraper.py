#!/usr/bin/env python3
"""
ì›¹ ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python scripts/test_scraper.py "https://example.com/jobs/123"
    python scripts/test_scraper.py "https://example.com/jobs/123" --no-ocr
    python scripts/test_scraper.py "https://example.com/jobs/123" --output result.txt
"""

import argparse
import asyncio
import base64
import re
import sys
from pathlib import Path
from urllib.parse import urljoin, urlparse

import httpx
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

load_dotenv()

import google.generativeai as genai
import os

# Gemini ì„¤ì •
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")


class WebScraperWithOCR:
    """ì´ë¯¸ì§€ OCRì„ í¬í•¨í•œ ì›¹ ìŠ¤í¬ë˜í¼"""

    def __init__(self, enable_ocr: bool = True):
        self.enable_ocr = enable_ocr
        self.client = httpx.AsyncClient(
            timeout=30.0,
            follow_redirects=True,
            headers={
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
            }
        )

        if enable_ocr and GOOGLE_API_KEY:
            genai.configure(api_key=GOOGLE_API_KEY)
            self.vision_model = genai.GenerativeModel("gemini-2.0-flash-exp")
        else:
            self.vision_model = None

    async def scrape(self, url: str) -> dict:
        """
        URLì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì´ë¯¸ì§€ OCR í¬í•¨)

        Returns:
            {
                "url": str,
                "html_text": str,
                "image_texts": list[dict],
                "combined_text": str,
                "metadata": dict
            }
        """
        print(f"\nğŸ” ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")

        # 1. HTML ê°€ì ¸ì˜¤ê¸°
        try:
            response = await self.client.get(url)
            response.raise_for_status()
            html = response.text
            print(f"âœ… HTML ê°€ì ¸ì˜¤ê¸° ì„±ê³µ ({len(html):,} bytes)")
        except Exception as e:
            return {"error": f"HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}", "url": url}

        # 2. HTML í…ìŠ¤íŠ¸ ì¶”ì¶œ
        soup = BeautifulSoup(html, "html.parser")

        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()

        html_text = self._extract_text(soup)
        print(f"âœ… HTML í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(html_text):,} chars)")

        # 3. ì´ë¯¸ì§€ URL ìˆ˜ì§‘
        image_urls = self._extract_image_urls(soup, url)
        print(f"ğŸ“· ì´ë¯¸ì§€ ë°œê²¬: {len(image_urls)}ê°œ")

        # 4. ì´ë¯¸ì§€ OCR (ì˜µì…˜)
        image_texts = []
        if self.enable_ocr and self.vision_model and image_urls:
            print(f"ğŸ”„ ì´ë¯¸ì§€ OCR ì§„í–‰ ì¤‘...")
            image_texts = await self._ocr_images(image_urls)
            ocr_count = sum(1 for t in image_texts if t.get("text"))
            print(f"âœ… OCR ì™„ë£Œ: {ocr_count}/{len(image_urls)}ê°œ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ")

        # 5. í…ìŠ¤íŠ¸ í†µí•©
        combined_text = self._combine_texts(html_text, image_texts)

        return {
            "url": url,
            "html_text": html_text,
            "image_texts": image_texts,
            "combined_text": combined_text,
            "metadata": {
                "html_length": len(html_text),
                "image_count": len(image_urls),
                "ocr_success_count": sum(1 for t in image_texts if t.get("text")),
                "combined_length": len(combined_text),
            }
        }

    def _extract_text(self, soup: BeautifulSoup) -> str:
        """HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = soup.get_text(separator="\n", strip=True)

        # ì—°ì† ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n{3,}', '\n\n', text)
        text = re.sub(r' {2,}', ' ', text)

        return text.strip()

    def _extract_image_urls(self, soup: BeautifulSoup, base_url: str) -> list[str]:
        """ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        image_urls = []

        for img in soup.find_all("img"):
            src = img.get("src") or img.get("data-src") or img.get("data-lazy-src")
            if not src:
                continue

            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ
            full_url = urljoin(base_url, src)

            # í•„í„°ë§: ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€, ì•„ì´ì½˜ ë“± ì œì™¸
            if self._is_valid_image_url(full_url, img):
                image_urls.append(full_url)

        return list(set(image_urls))  # ì¤‘ë³µ ì œê±°

    def _is_valid_image_url(self, url: str, img_tag) -> bool:
        """ìœ íš¨í•œ ì´ë¯¸ì§€ì¸ì§€ í™•ì¸"""
        # í™•ì¥ì ì²´í¬
        valid_extensions = ('.jpg', '.jpeg', '.png', '.webp', '.gif')
        parsed = urlparse(url)

        # base64 ì´ë¯¸ì§€
        if url.startswith("data:image"):
            return True

        # íŒŒì¼ í™•ì¥ì ë˜ëŠ” ì´ë¯¸ì§€ ì„œë¹„ìŠ¤ URL
        if not any(parsed.path.lower().endswith(ext) for ext in valid_extensions):
            # ì´ë¯¸ì§€ ì„œë¹„ìŠ¤ URL íŒ¨í„´
            if not any(x in url for x in ['image', 'img', 'photo', 'pic', 'cdn']):
                return False

        # í¬ê¸° ì²´í¬ (ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ ì œì™¸)
        width = img_tag.get("width", "0")
        height = img_tag.get("height", "0")
        try:
            if int(width) < 100 or int(height) < 100:
                return False
        except (ValueError, TypeError):
            pass  # í¬ê¸° ì •ë³´ ì—†ìœ¼ë©´ ì¼ë‹¨ í¬í•¨

        # ì•„ì´ì½˜/ë¡œê³  íŒ¨í„´ ì œì™¸
        skip_patterns = ['icon', 'logo', 'avatar', 'emoji', 'button', 'arrow']
        url_lower = url.lower()
        if any(p in url_lower for p in skip_patterns):
            return False

        return True

    async def _ocr_images(self, image_urls: list[str]) -> list[dict]:
        """ì´ë¯¸ì§€ë“¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Gemini Vision)"""
        results = []

        for i, url in enumerate(image_urls[:10]):  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
            try:
                print(f"  OCR [{i+1}/{min(len(image_urls), 10)}]: {url[:60]}...")

                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                if url.startswith("data:image"):
                    # base64 ì´ë¯¸ì§€
                    image_data = base64.b64decode(url.split(",")[1])
                else:
                    response = await self.client.get(url)
                    response.raise_for_status()
                    image_data = response.content

                # Gemini Visionìœ¼ë¡œ OCR
                text = await self._gemini_ocr(image_data)

                results.append({
                    "url": url,
                    "text": text,
                    "success": bool(text)
                })

            except Exception as e:
                results.append({
                    "url": url,
                    "text": "",
                    "success": False,
                    "error": str(e)
                })

        return results

    async def _gemini_ocr(self, image_data: bytes) -> str:
        """Gemini Visionìœ¼ë¡œ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # ì´ë¯¸ì§€ë¥¼ Partë¡œ ë³€í™˜
            image_part = {
                "mime_type": "image/jpeg",
                "data": base64.b64encode(image_data).decode()
            }

            prompt = """ì´ ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ê·œì¹™:
- ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì¶”ì¶œ
- ë ˆì´ì•„ì›ƒ/ìˆœì„œë¥¼ ìµœëŒ€í•œ ìœ ì§€
- í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
- ì„¤ëª…ì´ë‚˜ í•´ì„ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥"""

            response = self.vision_model.generate_content([prompt, image_part])

            return response.text.strip() if response.text else ""

        except Exception as e:
            print(f"    âš ï¸ OCR ì˜¤ë¥˜: {e}")
            return ""

    def _combine_texts(self, html_text: str, image_texts: list[dict]) -> str:
        """HTML í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ OCR í…ìŠ¤íŠ¸ í†µí•©"""
        parts = [html_text]

        ocr_texts = [t["text"] for t in image_texts if t.get("text")]
        if ocr_texts:
            parts.append("\n\n" + "=" * 50)
            parts.append("[ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸]")
            parts.append("=" * 50 + "\n")
            parts.extend(ocr_texts)

        return "\n\n".join(parts)

    async def close(self):
        await self.client.aclose()


async def main():
    parser = argparse.ArgumentParser(description="ì›¹ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì´ë¯¸ì§€ OCR í¬í•¨)")
    parser.add_argument("url", help="ìŠ¤í¬ë˜í•‘í•  URL")
    parser.add_argument("--no-ocr", action="store_true", help="ì´ë¯¸ì§€ OCR ë¹„í™œì„±í™”")
    parser.add_argument("--output", "-o", help="ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ")

    args = parser.parse_args()

    # API í‚¤ í™•ì¸
    if not args.no_ocr and not GOOGLE_API_KEY:
        print("âš ï¸ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. OCR ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
        args.no_ocr = True

    scraper = WebScraperWithOCR(enable_ocr=not args.no_ocr)

    try:
        result = await scraper.scrape(args.url)

        if "error" in result:
            print(f"\nâŒ ì˜¤ë¥˜: {result['error']}")
            return

        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ“„ ì¶”ì¶œ ê²°ê³¼")
        print("=" * 60)
        print(f"\nğŸ“Š ë©”íƒ€ë°ì´í„°:")
        print(f"   - HTML í…ìŠ¤íŠ¸: {result['metadata']['html_length']:,} chars")
        print(f"   - ì´ë¯¸ì§€ ìˆ˜: {result['metadata']['image_count']}ê°œ")
        print(f"   - OCR ì„±ê³µ: {result['metadata']['ocr_success_count']}ê°œ")
        print(f"   - ì´ í…ìŠ¤íŠ¸: {result['metadata']['combined_length']:,} chars")

        print("\n" + "-" * 60)
        print("ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:")
        print("-" * 60)
        print(result["combined_text"][:5000])  # ì²˜ìŒ 5000ìë§Œ ì¶œë ¥

        if len(result["combined_text"]) > 5000:
            print(f"\n... (ì´ {len(result['combined_text']):,}ì ì¤‘ 5000ìë§Œ í‘œì‹œ)")

        # íŒŒì¼ ì €ì¥ (ì˜µì…˜)
        if args.output:
            output_path = Path(args.output)
            output_path.write_text(result["combined_text"], encoding="utf-8")
            print(f"\nâœ… ê²°ê³¼ ì €ì¥ë¨: {output_path}")

    finally:
        await scraper.close()


if __name__ == "__main__":
    asyncio.run(main())
