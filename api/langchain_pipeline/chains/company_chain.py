"""
íšŒì‚¬ ì»¬ì³í• ë¶„ì„ ì²´ì¸

íŒŒì´í”„ë¼ì¸ íë¦„:
ì±„ìš©ê³µê³  URL â†’ Playwright ìŠ¤í¬ë˜í•‘ â†’ íšŒì‚¬ ë§¤ì¹­ â†’ ì¶”ê°€ URL ìŠ¤í¬ë˜í•‘ â†’
ë°ì´í„° ìˆ˜ì§‘ â†’ ì»¬ì³í• ë¶„ì„ â†’ MongoDB ì €ì¥

ì§€ì› íšŒì‚¬: í˜„ëŒ€ì˜¤í† ì—ë²„, ì—…ìŠ¤í…Œì´ì§€, í† ìŠ¤
"""

import json
import re
from typing import Any, Optional

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser


def parse_json_with_markdown(text: str) -> dict:
    """ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ì´ í¬í•¨ëœ JSON íŒŒì‹±"""
    # AIMessageì¸ ê²½ìš° content ì¶”ì¶œ
    if hasattr(text, 'content'):
        text = text.content

    text = str(text).strip()

    # ```json ... ``` ë˜ëŠ” ``` ... ``` ì œê±°
    if "```json" in text:
        match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if match:
            text = match.group(1)
    elif "```" in text:
        match = re.search(r'```\s*(.*?)\s*```', text, re.DOTALL)
        if match:
            text = match.group(1)

    return json.loads(text)

from api.langchain_pipeline.config import (
    GOOGLE_API_KEY,
    COMPANY_KEYWORDS,
    match_company,
    get_company_sources,
)
from api.langchain_pipeline.scrapers.browser_scraper import BrowserScraper
from api.langchain_pipeline.utils.schema_loader import get_schema_for_prompt
from api.langchain_pipeline.utils.db_handler import DatabaseHandler
from api.langchain_pipeline.prompts import company_data_collect, company_culture_analyze


class UnsupportedCompanyError(Exception):
    """ì§€ì›í•˜ì§€ ì•ŠëŠ” íšŒì‚¬ ì—ëŸ¬"""
    pass


class CompanyAnalysisChain:
    """íšŒì‚¬ ì»¬ì³í• ë¶„ì„ ì²´ì¸"""

    SUPPORTED_COMPANIES = list(COMPANY_KEYWORDS.keys())

    def __init__(
        self,
        model_name: str = "gemini-2.5-flash",
        temperature: float = 0.0,
        save_to_db: bool = True
    ):
        """
        Args:
            model_name: Gemini ëª¨ë¸ëª…
            temperature: ìƒì„± ì˜¨ë„
            save_to_db: DB ì €ì¥ ì—¬ë¶€
        """
        self.llm = ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=GOOGLE_API_KEY,
            temperature=temperature,
        )
        self.scraper = BrowserScraper(headless=True)
        self.save_to_db = save_to_db
        self.db = DatabaseHandler() if save_to_db else None

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        self._setup_prompts()

    def _setup_prompts(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”"""
        # ë°ì´í„° ìˆ˜ì§‘ í”„ë¡¬í”„íŠ¸
        self.collect_prompt = ChatPromptTemplate.from_messages([
            ("system", company_data_collect.SYSTEM_MESSAGE),
            ("human", company_data_collect.HUMAN_MESSAGE_TEMPLATE),
        ])

        # ì»¬ì³í• ë¶„ì„ í”„ë¡¬í”„íŠ¸
        self.analyze_prompt = ChatPromptTemplate.from_messages([
            ("system", company_culture_analyze.SYSTEM_MESSAGE),
            ("human", company_culture_analyze.HUMAN_MESSAGE_TEMPLATE),
        ])

        # JSON íŒŒì„œ
        self.json_parser = JsonOutputParser()

    async def scrape_urls(self, urls: list[str]) -> str:
        """
        URLë“¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ

        Args:
            urls: ìŠ¤í¬ë˜í•‘í•  URL ë¦¬ìŠ¤íŠ¸

        Returns:
            ê²°í•©ëœ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ í…ìŠ¤íŠ¸
        """
        results = await self.scraper.scrape_multiple(urls)
        contents = []

        for result in results:
            if result.success:
                contents.append(f"=== {result.url} ===\n{result.content}")
            else:
                contents.append(f"=== {result.url} (ì‹¤íŒ¨) ===\n{result.error_message}")

        return "\n\n".join(contents)

    async def collect_company_data(self, scraped_content: str) -> dict[str, Any]:
        """
        ìŠ¤í¬ë˜í•‘ ê²°ê³¼ì—ì„œ íšŒì‚¬ ë°ì´í„° ìˆ˜ì§‘

        Args:
            scraped_content: ìŠ¤í¬ë˜í•‘ëœ í…ìŠ¤íŠ¸

        Returns:
            êµ¬ì¡°í™”ëœ íšŒì‚¬ ë°ì´í„°
        """
        schema = get_schema_for_prompt("company_schema")

        chain = self.collect_prompt | self.llm

        response = await chain.ainvoke({
            "scraped_content": scraped_content,
            "output_schema": schema,
        })

        return parse_json_with_markdown(response)

    async def analyze_culture(self, company_data: dict[str, Any]) -> dict[str, Any]:
        """
        íšŒì‚¬ ë°ì´í„° ê¸°ë°˜ ì»¬ì³í• ë¶„ì„

        Args:
            company_data: ìˆ˜ì§‘ëœ íšŒì‚¬ ë°ì´í„°

        Returns:
            ì»¬ì³í• ë¶„ì„ ê²°ê³¼
        """
        schema = get_schema_for_prompt("company_schema")

        chain = self.analyze_prompt | self.llm

        response = await chain.ainvoke({
            "company_data": json.dumps(company_data, ensure_ascii=False, indent=2),
            "output_schema": schema,
        })

        return parse_json_with_markdown(response)

    async def run(
        self,
        job_posting_url: str,
    ) -> dict[str, Any]:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            job_posting_url: ì±„ìš©ê³µê³  URL

        Returns:
            ìµœì¢… ë¶„ì„ ê²°ê³¼

        Raises:
            UnsupportedCompanyError: ì§€ì›í•˜ì§€ ì•ŠëŠ” íšŒì‚¬ì¸ ê²½ìš°
        """
        print(f"ğŸ“ ì±„ìš©ê³µê³  URL: {job_posting_url}")

        # 1. ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í•‘
        print("ğŸ”„ ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í•‘ ì¤‘...")
        job_result = await self.scraper.scrape(job_posting_url)

        if not job_result.success:
            raise Exception(f"ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {job_result.error_message}")

        job_content = job_result.content

        # 2. íšŒì‚¬ ë§¤ì¹­
        company_name = match_company(job_content)

        if company_name is None:
            await self.scraper.close()
            raise UnsupportedCompanyError(
                f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íšŒì‚¬ì…ë‹ˆë‹¤. ì§€ì› íšŒì‚¬: {', '.join(self.SUPPORTED_COMPANIES)}"
            )

        print(f"ğŸ¢ íšŒì‚¬ ë§¤ì¹­: {company_name}")

        # 3. ì¶”ê°€ URL ìŠ¤í¬ë˜í•‘
        additional_urls = get_company_sources(company_name)
        print(f"ğŸ“„ ì¶”ê°€ ì†ŒìŠ¤ {len(additional_urls)}ê°œ ìŠ¤í¬ë˜í•‘ ì¤‘...")

        all_contents = [f"=== ì±„ìš©ê³µê³ : {job_posting_url} ===\n{job_content}"]

        for url in additional_urls:
            print(f"   - {url}")
            result = await self.scraper.scrape(url)
            if result.success:
                all_contents.append(f"=== {url} ===\n{result.content}")
            else:
                print(f"     âš ï¸ ì‹¤íŒ¨: {result.error_message}")

        await self.scraper.close()

        # 4. ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
        scraped_content = "\n\n".join(all_contents)
        print(f"ğŸ“ ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(scraped_content)} chars")

        # 5. íšŒì‚¬ ë°ì´í„° ìˆ˜ì§‘
        print("ğŸ” íšŒì‚¬ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        company_data = await self.collect_company_data(scraped_content)

        # 6. ì»¬ì³í• ë¶„ì„
        print("ğŸ“Š ì»¬ì³í• ë¶„ì„ ì¤‘...")
        culture_analysis = await self.analyze_culture(company_data)

        # ê²°ê³¼: ì»¬ì³í• ë¶„ì„ ê²°ê³¼ë§Œ ë°˜í™˜ (ì¤‘ë³µ ì œê±°)
        # culture_analysisì— ë©”íƒ€ ì •ë³´ ì¶”ê°€
        result = culture_analysis
        result["_meta"] = {
            "company_name": company_name,
            "job_posting_url": job_posting_url,
            "source_urls": [job_posting_url] + additional_urls,
        }

        # 7. DB ì €ì¥ (ì˜µì…˜)
        if self.save_to_db and self.db:
            doc_id = self.db.save_company_profile(result)
            result["_id"] = doc_id
            print(f"ğŸ’¾ DB ì €ì¥ ì™„ë£Œ: {doc_id}")

        print("âœ… ë¶„ì„ ì™„ë£Œ!")
        return result

    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.db:
            self.db.close()
